# -*- coding: utf-8 -*-
"""classification_speaker_independent.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TWbQ2PFQON7nc8FvAKfii3UtTKmM3f51

# VQ-MAE Speech Emotion Classification (Speaker Independent)

This notebook performs 5-fold cross-validation for speaker-independent emotion classification using VQ-MAE.
"""

# Import required libraries
from vqmae import MAE, SpeechVQVAE, Classifier_Train, EvaluationDataset, h5_creation, size_model
import hydra
from omegaconf import DictConfig
import os
import numpy as np
from sklearn.utils import shuffle
import warnings

warnings.filterwarnings('ignore')
import torch, random
# 1) device + seeds
device = "cuda" if torch.cuda.is_available() else "cpu"
seed = 42
random.seed(seed)
np.random.seed(seed)
torch.manual_seed(seed)
if device == "cuda":
    torch.cuda.manual_seed_all(seed)

# Configuration parameters
Total_folds = 5
root = r"/net/tscratch/people/plgmarbar/ravdess/ravdess"
dataset_name = "ravdess"
h5_path = r"/net/tscratch/people/plgmarbar/ravdess/H5/ravdess.hdf5"
mae_path = r"/net/tscratch/people/plgmarbar/ravdess/checkpoint/RSMAE/2023-2-22/12-45"

print(f"Dataset root: {root}")
print(f"H5 cache path: {h5_path}")
print(f"MAE model path: {mae_path}")

def fold_creation(list_id, num_fold, k=5):
    """Create fold for cross-validation"""
    n = len(list_id)
    base = n // k
    rest = n % k
    start = num_fold * base + min(num_fold, rest)
    stop  = start + base + (1 if num_fold < rest else 0)
    return list_id[start:stop]

# Load Hydra configuration
from hydra import initialize, compose

# Initialize Hydra
with initialize(config_path=f"config_mae"):
    cfg = compose(config_name="config")

print("Configuration loaded successfully!")
cfg.train["total_epoch"] = 5
cfg.train["warmup_epoch"]=5
print(f"Training config: {cfg.train}")
print(f"Model config: {cfg.model}")
print(f"VQ-VAE config: {cfg.vqvae}")
cfg.train["total_epoch"] = 5
# Create H5 directory if it doesn't exist
os.makedirs("H5", exist_ok=True)
print("H5 directory created/verified")

# Load VQ-VAE model
print("Loading VQ-VAE model...")
vqvae = SpeechVQVAE(**cfg.vqvae)
vqvae.load(path_model=r"/net/tscratch/people/plgmarbar/ravdess/checkpoint/SPEECH_VQVAE/model_checkpoint")
print("VQ-VAE model loaded successfully!")

# Load dataset

print("Loading RAVDESS dataset...")
dataset = EvaluationDataset(root=root,
                          speaker_retain_test=[],
                          frames_per_clip=200,
                          dataset=dataset_name,
                          )
print(f"Dataset loaded: {len(dataset.table)} samples")
print(f"Speakers: {dataset.table['id'].nunique()}")
print(f"Emotions: {dataset.table['emotion'].nunique()}")

# Create H5 cache file if it doesn't exist
if not os.path.exists(h5_path):
    print("Creating H5 cache file... (This may take a while)")
    h5_creation(vqvae, dataset=dataset, dir_save=h5_path)
    print("H5 cache created successfully!")
else:
    print("Using existing H5 cache file...")

# Get unique speaker IDs for cross-validation
all_id = shuffle(np.unique(np.array(dataset.table["id"])))
print(f"Total speakers: {len(all_id)}")
print(f"Speakers per fold: {len(all_id) // Total_folds}")
print(f"Speakers: {all_id[:10]}...")  # Show first 10 speakers

import torch


# Initialize results storage
accuracy_epoch = []
f1_epoch = []

print(f"Starting {Total_folds}-fold cross-validation...")

# FOLD 1: Create train/test split
num_fold = 0
print(f"\n=== FOLD {num_fold + 1}/{Total_folds} ===")

speaker_retain_test = fold_creation(list(all_id), num_fold=num_fold, k=Total_folds)
print(f"Test speakers: {speaker_retain_test}")
print(f"Number of test speakers: {len(speaker_retain_test)}")

# Create training dataset
print("Creating training dataset...")
data_train = EvaluationDataset(root=root,
                               speaker_retain_test=speaker_retain_test,
                               train=True,
                               frames_per_clip=200,
                               dataset=dataset_name,
                               h5_path=h5_path
                               )
print(f"Training samples: {len(data_train.table)}")

# Create validation dataset
print("Creating validation dataset...")
data_validation = EvaluationDataset(root=root,
                                    speaker_retain_test=speaker_retain_test,
                                    train=False,
                                    frames_per_clip=200,
                                    dataset=dataset_name,
                                    h5_path=h5_path
                                    )
print(f"Validation samples: {len(data_validation.table)}")

# Load MAE model
print("Loading MAE model...")
#checkpoint = torch.load(f"{mae_path}/model_checkpoint", map_location='cpu')
#state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
#model.load_state_dict(state_dict, strict=False)

#mae = MAE(**cfg.model, trainable_position=True)
#mae.load(path_model=f"{mae_path}/model_checkpoint")
#size_model(mae, "mae")
mae = MAE(**cfg.model, trainable_position=True)

# Load checkpoint safely (ignore unexpected/missing keys)
checkpoint = torch.load(f"{mae_path}/model_checkpoint", map_location='cpu')
state_dict = checkpoint.get('model_state_dict', checkpoint)  # handle dict-wrapped checkpoints
mae.load_state_dict(state_dict, strict=False)  # strict=False skips mismatched keys

# Optional: print summary of loaded model
size_model(mae, "mae")
print("MAE model loaded successfully (partial load if keys mismatched)!")

print("MAE model loaded successfully!")



# Initialize classifier training
print("Initializing classifier training...")
pretrain_classifier = Classifier_Train(mae,
                                       vqvae,
                                       data_train,
                                       data_validation,
                                       config_training=cfg.train,
                                       follow=True,
                                       query2emo=False)
print("Classifier training initialized!")



# Train classifier for fold 1
print("Starting training for fold 1...")
accuracy, f1 = pretrain_classifier.fit()
accuracy_epoch.append(accuracy)
f1_epoch.append(f1)

print(f"Fold 1 Results:")
print(f"  Accuracy: {accuracy:.4f}")
print(f"  F1 Score: {f1:.4f}")

# Continue with remaining folds (2, 3, 4, 5)
# You can copy the pattern above for each fold
# Or create a loop for the remaining folds

for num_fold in range(1, Total_folds):
    print(f"\n=== FOLD {num_fold + 1}/{Total_folds} ===")

    speaker_retain_test = fold_creation(list(all_id), num_fold=num_fold, k=Total_folds)
    print(f"Test speakers: {speaker_retain_test}")

    # Create datasets
    data_train = EvaluationDataset(root=root,
                                   speaker_retain_test=speaker_retain_test,
                                   train=True,
                                   frames_per_clip=200,
                                   dataset=dataset_name,
                                   h5_path=h5_path
                                   )

    data_validation = EvaluationDataset(root=root,
                                        speaker_retain_test=speaker_retain_test,
                                        train=False,
                                        frames_per_clip=200,
                                        dataset=dataset_name,
                                        h5_path=h5_path
                                        )

    # Load MAE model
    mae = MAE(**cfg.model, trainable_position=True)
    mae.load(path_model=f"{mae_path}/model_checkpoint")

    # Train classifier
    pretrain_classifier = Classifier_Train(mae,
                                           vqvae,
                                           data_train,
                                           data_validation,
                                           config_training=cfg.train,
                                           follow=True,
                                           query2emo=False)

    print(f"Starting training for fold {num_fold + 1}...")
    accuracy, f1 = pretrain_classifier.fit()
    accuracy_epoch.append(accuracy)
    f1_epoch.append(f1)

    print(f"Fold {num_fold + 1} Results:")
    print(f"  Accuracy: {accuracy:.4f}")
    print(f"  F1 Score: {f1:.4f}")
    print("-" * 50)

# Final results
print("\n" + "="*60)
print("FINAL RESULTS")
print("="*60)
print(f"Final accuracy score: {np.mean(accuracy_epoch):.4f} ± {np.std(accuracy_epoch):.4f}")
print(f"Final F1 score: {np.mean(f1_epoch):.4f} ± {np.std(f1_epoch):.4f}")
print("\nPer-fold results:")
for i, (acc, f1) in enumerate(zip(accuracy_epoch, f1_epoch)):
    print(f"  Fold {i+1}: Accuracy={acc:.4f}, F1={f1:.4f}")
